{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 1: Classification of Iris Flowers\n",
    "Input: Iris.csv data set\n",
    "Project: Building different classification models, validation and performance\n",
    "evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import all necessary libraries\n",
    "    The following libraries to be imported in this project:\n",
    "    pandas: Used to read and manipulate CSV data.\n",
    "    Numpy: For fast and efficient processing of data\n",
    "    sklearn.dataset: To load data from the Sci-Kit-Learn repository\n",
    "    sklearn.train_test_split: From scikit-learn, used to split data into training and testing sets.\n",
    "    sklearn.preprocessing: For feature scaling/normalization\n",
    "    sklearn.LogisticRegression: A common classification algorithm from scikit-learn.\n",
    "    sklearn.SVC: Support Vector Machine Classifier\n",
    "    sklearn.RandomeForest: Random Forest Classification\n",
    "    sklearn.KNeighborsClassifier: k-Nearest Neighbour classifier\n",
    "    sklearn.DecissionTreeClassifier: Decision Tree Classifier\n",
    "    sklearn.MLPClassifier: Multi-Layer Perceptron classifier\n",
    "    sklearn.GradientBoostingClassifier: Gradient Boosting classifier\n",
    "    sklearn.accuracy_score: To calculate model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      " 4   target             150 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 5.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "# Step 1: Load the Iris dataset (it is a classic builtin dataset)\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target\n",
    "# Download the data from \"Iris.csv\"   locally             \n",
    "X, y = iris.data, iris.target        \n",
    "# Convert to DataFrame for better processing\n",
    "df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "# Preview the dataset: It is required as a customary step!\n",
    "#print(\"Top 5 rows of the dataset:\")\n",
    "#print(df.head())\n",
    "#print(\"Bottom 5 rows of the dataset:\")\n",
    "#print(df.tail())\n",
    "#print(\"The columns present in the data frame\n",
    "#print(df.columns)\n",
    "#print(\"The information about the attributes\n",
    "print(df.info())\n",
    "#print(\"To check if the null entries are there\")\n",
    "#print(df.isnull())\n",
    "#print(\"The statistical information about the data\")\n",
    "# print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Split the data set into two parts: \"Training set\" and \"Test set\"\n",
    "The following library is used\n",
    "    import train_test_split from sklearn.model_selection\n",
    "    \"Training set\" is used to train a model and \"Test set\" is used to test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import of \"Train-Test-Split-Selection\" library is successful\n",
      "\n",
      "Train and test data shapes:\n",
      "X_train: (100, 4) X_test: (50, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Import of \\\"Train-Test-Split-Selection\\\" library is successful\")\n",
    "\n",
    "# Split the dataset into training and testing sets: 67% for training and 33% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, train_size=0.67)\n",
    "\n",
    "# Note 1: Data (i.e., Data-attributes and Target-column) are kept as separate variables (X for features, y for target labels)\n",
    "# Note 2: Here, random_state=42 is chosen as a seed value and popularly used for reproducibility in experiments\n",
    "\n",
    "print(\"\\nTrain and test data shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Preprocessing\n",
    "    (a)Handling null-entries, if applicable\n",
    "    (b) Scaling (to put all values in a normalize scale)\n",
    "        \n",
    "        For scaling there are many methods: StandardScalar, MinMaxScalar, Normalizer, PolynomialFeatures, etc. Use any one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized training dataset...\n",
      "\n",
      "[[-1.72849788  0.40824829]\n",
      " [-0.83223972  0.81649658]\n",
      " [-0.38411064  1.63299316]\n",
      " [ 0.06401844 -1.63299316]\n",
      " [ 0.51214752 -0.81649658]\n",
      " [ 0.9602766  -0.40824829]\n",
      " [ 1.40840568  0.        ]]\n",
      "\n",
      "Normalized testing dataset...\n",
      "\n",
      "[[-2.17662696  1.22474487]\n",
      " [ 0.06401844 -0.40824829]\n",
      " [-1.2803688  -1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Tutorial to learn the basics of scalar-based normalization\n",
    "\n",
    "# Create a DataFrame for training data\n",
    "data1 = {'A': [2, 4, 5, 6, 7, 8, 9], 'B': [60, 70, 90, 10, 30, 40, 50]}\n",
    "# Create a DataFrame for testing data\n",
    "data2 = {'A': [1, 6, 3], 'B': [80, 40, 20]}\n",
    "\n",
    "# Convert dictionaries to pandas DataFrame\n",
    "X_train_ = pd.DataFrame(data1)\n",
    "X_test_ = pd.DataFrame(data2)\n",
    "\n",
    "# Create a StandardScaler object for normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data (learn scaling parameters from training data)\n",
    "X_train_scaled_ = scaler.fit_transform(X_train_)\n",
    "print(\"Normalized training dataset...\\n\")\n",
    "print(X_train_scaled_)  # Using print instead of display() for general compatibility\n",
    "\n",
    "# Transform the testing data using the parameters already learned from training data\n",
    "X_test_scaled_ = scaler.transform(X_test_)  # Corrected from fit_transform() to transform()\n",
    "print(\"\\nNormalized testing dataset...\\n\")\n",
    "print(X_test_scaled_)  # Using print instead of display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaled Data (First 5 rows):\n",
      " [[-0.70686204  2.40241739 -1.17067786 -1.33850041]\n",
      " [-1.0608831  -1.54739844 -0.18183687 -0.18240601]\n",
      " [-1.0608831   0.07899632 -1.17067786 -1.33850041]\n",
      " [-0.11682695  3.09944372 -1.17067786 -0.95313561]\n",
      " [ 0.59121516 -0.61803001  0.80700412  0.45986866]]\n",
      "\n",
      "Min-Max Scaled Data (First 5 rows):\n",
      " [[0.26470588 0.86363636 0.08474576 0.        ]\n",
      " [0.17647059 0.09090909 0.38983051 0.375     ]\n",
      " [0.17647059 0.40909091 0.08474576 0.        ]\n",
      " [0.41176471 1.         0.08474576 0.125     ]\n",
      " [0.58823529 0.27272727 0.69491525 0.58333333]]\n",
      "\n",
      "Normalized Data (First 5 rows):\n",
      " [[0.76578311 0.60379053 0.22089897 0.0147266 ]\n",
      " [0.75916547 0.37183615 0.51127471 0.15493173]\n",
      " [0.81803119 0.51752994 0.25041771 0.01669451]\n",
      " [0.77381111 0.59732787 0.2036345  0.05430253]\n",
      " [0.72366005 0.32162669 0.58582004 0.17230001]]\n",
      "\n",
      "Polynomial Features (First 5 rows):\n",
      " [[5.200e+00 4.100e+00 1.500e+00 1.000e-01 2.704e+01 2.132e+01 7.800e+00\n",
      "  5.200e-01 1.681e+01 6.150e+00 4.100e-01 2.250e+00 1.500e-01 1.000e-02]\n",
      " [4.900e+00 2.400e+00 3.300e+00 1.000e+00 2.401e+01 1.176e+01 1.617e+01\n",
      "  4.900e+00 5.760e+00 7.920e+00 2.400e+00 1.089e+01 3.300e+00 1.000e+00]\n",
      " [4.900e+00 3.100e+00 1.500e+00 1.000e-01 2.401e+01 1.519e+01 7.350e+00\n",
      "  4.900e-01 9.610e+00 4.650e+00 3.100e-01 2.250e+00 1.500e-01 1.000e-02]\n",
      " [5.700e+00 4.400e+00 1.500e+00 4.000e-01 3.249e+01 2.508e+01 8.550e+00\n",
      "  2.280e+00 1.936e+01 6.600e+00 1.760e+00 2.250e+00 6.000e-01 1.600e-01]\n",
      " [6.300e+00 2.800e+00 5.100e+00 1.500e+00 3.969e+01 1.764e+01 3.213e+01\n",
      "  9.450e+00 7.840e+00 1.428e+01 4.200e+00 2.601e+01 7.650e+00 2.250e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values: There are no missing values\n",
    "\n",
    "# Normalization of training and testing data\n",
    "\n",
    "'''\n",
    "Note: For normalization, sklearn provides two methods: fit_transform() and transform().\n",
    "    - fit_transform() is applied to training data, whereas transform() is applied to testing data.\n",
    "    - fit_transform() is a combination of:\n",
    "    - fit(): To calculate the necessary transformation parameters based on the training data (e.g., min, max, mean, standard deviation).\n",
    "    - transform(): To apply the transformation to the data using the parameters learned from the training data.\n",
    "    - The two methods are applicable to all normalization methods defined in sklearn.\n",
    "'''\n",
    "\n",
    "# Import scaling methods for normalization\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, PolynomialFeatures\n",
    "\n",
    "# Standard Scaling (Standardization)\n",
    "scaler = StandardScaler()  # Create a StandardScaler object\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply transform() to test data using learned parameters\n",
    "print(\"Standard Scaled Data (First 5 rows):\\n\", X_train_scaled[:5])  # Show first 5 rows\n",
    "\n",
    "# Min-Max Scaling (Normalization to range [0,1])\n",
    "minmax_scaler = MinMaxScaler()  # Create a MinMaxScaler object\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_minmax = minmax_scaler.transform(X_test)  # Apply transform() to test data using learned parameters\n",
    "print(\"\\nMin-Max Scaled Data (First 5 rows):\\n\", X_train_minmax[:5])  # Show first 5 rows\n",
    "\n",
    "# L2 Normalization (Scaling each row to unit norm)\n",
    "normalizer = Normalizer()  # Create a Normalizer object\n",
    "X_train_normalized = normalizer.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_normalized = normalizer.transform(X_test)  # Apply transform() to test data using learned parameters\n",
    "print(\"\\nNormalized Data (First 5 rows):\\n\", X_train_normalized[:5])  # Show first 5 rows\n",
    "\n",
    "# Polynomial Feature Transformation (Expanding features up to the given degree)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)  # Create a PolynomialFeatures object (degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)  # Fit and transform the training data\n",
    "X_test_poly = poly.transform(X_test)  # Apply transform() to test data using learned parameters\n",
    "print(\"\\nPolynomial Features (First 5 rows):\\n\", X_train_poly[:5])  # Show first 5 rows\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
